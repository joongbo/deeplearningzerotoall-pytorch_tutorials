{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "### y = Wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### 모델 정의 ###########\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "hypothesis = x_train * W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### cost (loss) ###########\n",
    "# e.g., MSE (mean squared error)\n",
    "cost = torch.mean((hypothesis - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### optimizer 설정 ###########\n",
    "optimizer = torch.optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "########### 아래 셋은 항상 같이 다니는 것! ###########\n",
    "optimizer.zero_grad() # gradient 초기화\n",
    "cost.backward() # gradient 계산\n",
    "optimizer.step() # update\n",
    "\n",
    "########### 다음을 계산 ###########\n",
    "# gradient = 2 * torch.mean((W * x_train + b - y_train ) * x_train)\n",
    "# W -= 0.01 * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1867], requires_grad=True) tensor([0.0800], requires_grad=True) tensor([[0.2667],\n",
      "        [0.4533],\n",
      "        [0.6400]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(W, b, x_train * W + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "########### 한번 ###########\n",
    "# 데이터 정의\n",
    "# hypothesis (parameters) 정의\n",
    "# optimizer 정의\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([W, b], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### 반복 ###########\n",
    "# hypothesis 계산 (예측)\n",
    "# cost 계산\n",
    "# optimizer 로 학습\n",
    "\n",
    "n_epoch = 1000\n",
    "for epoch in range(n_epoch):\n",
    "    hypothesis = x_train * W + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9708], requires_grad=True) tensor([0.0664], requires_grad=True) tensor([[2.0372],\n",
      "        [4.0080],\n",
      "        [5.9788]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(W, b, x_train * W + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70],])\n",
    "y_train = torch.FloatTensor([152, 185, 180, 196, 142])\n",
    "\n",
    "W = torch.zeros(3, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5) # 0.00001\n",
    "\n",
    "print(x_train.shape)\n",
    "print(W.shape)\n",
    "print(x_train.matmul(W).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0.]) 29661.80078125\n",
      "tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) 9298.5205078125\n",
      "tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) 2915.71240234375\n",
      "tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) 915.04052734375\n",
      "tensor([137.7967, 165.6247, 163.1911, 177.7112, 126.3307]) 287.93609619140625\n",
      "tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) 91.3710708618164\n",
      "tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812]) 29.758249282836914\n",
      "tensor([150.1744, 180.5042, 177.8509, 193.6753, 137.6805]) 10.445266723632812\n",
      "tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) 4.391237258911133\n",
      "tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) 2.493121385574341\n",
      "tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) 1.8976876735687256\n",
      "tensor([152.5485, 183.3609, 180.6640, 196.7389, 139.8602]) 1.7105515003204346\n",
      "tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) 1.6514164209365845\n",
      "tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) 1.632369041442871\n",
      "tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) 1.625923752784729\n",
      "tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) 1.6234203577041626\n",
      "tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) 1.6221520900726318\n",
      "tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) 1.6212613582611084\n",
      "tensor([152.7999, 183.6688, 180.9644, 197.0661, 140.0963]) 1.6205012798309326\n",
      "tensor([152.8014, 183.6715, 180.9665, 197.0686, 140.0985]) 1.6197574138641357\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 20\n",
    "for epoch in range(n_epoch):\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    print(hypothesis.squeeze().detach(), cost.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70],])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "W = torch.zeros((3,1), requires_grad=True) # W = torch.zeros(3, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5) # 0.00001\n",
    "\n",
    "print(x_train.shape)\n",
    "print(W.shape)\n",
    "print(x_train.matmul(W).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([152.7948, 183.6807, 180.9668, 197.0694, 140.1094]) 1.6049751043319702\n",
      "tensor([152.7944, 183.6810, 180.9666, 197.0693, 140.1098]) 1.6042795181274414\n",
      "tensor([152.7939, 183.6814, 180.9665, 197.0692, 140.1102]) 1.603571891784668\n",
      "tensor([152.7935, 183.6817, 180.9664, 197.0691, 140.1106]) 1.602870225906372\n",
      "tensor([152.7931, 183.6819, 180.9663, 197.0690, 140.1110]) 1.602174162864685\n",
      "tensor([152.7926, 183.6822, 180.9661, 197.0688, 140.1114]) 1.6014728546142578\n",
      "tensor([152.7922, 183.6825, 180.9660, 197.0687, 140.1118]) 1.60076105594635\n",
      "tensor([152.7918, 183.6828, 180.9659, 197.0686, 140.1122]) 1.6000869274139404\n",
      "tensor([152.7913, 183.6831, 180.9657, 197.0685, 140.1126]) 1.599369764328003\n",
      "tensor([152.7909, 183.6834, 180.9656, 197.0684, 140.1130]) 1.5986852645874023\n",
      "tensor([152.7905, 183.6837, 180.9655, 197.0683, 140.1134]) 1.5979793071746826\n",
      "tensor([152.7901, 183.6840, 180.9653, 197.0682, 140.1138]) 1.5972920656204224\n",
      "tensor([152.7896, 183.6843, 180.9652, 197.0681, 140.1143]) 1.5965895652770996\n",
      "tensor([152.7892, 183.6846, 180.9651, 197.0679, 140.1147]) 1.5958976745605469\n",
      "tensor([152.7888, 183.6849, 180.9650, 197.0678, 140.1151]) 1.5952110290527344\n",
      "tensor([152.7883, 183.6852, 180.9648, 197.0677, 140.1155]) 1.5945138931274414\n",
      "tensor([152.7879, 183.6855, 180.9647, 197.0676, 140.1159]) 1.5938066244125366\n",
      "tensor([152.7875, 183.6858, 180.9646, 197.0675, 140.1163]) 1.5931155681610107\n",
      "tensor([152.7870, 183.6861, 180.9644, 197.0674, 140.1167]) 1.5924264192581177\n",
      "tensor([152.7866, 183.6864, 180.9643, 197.0673, 140.1171]) 1.591728925704956\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 20\n",
    "for epoch in range(n_epoch):\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    print(hypothesis.squeeze().detach(), cost.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cautions for Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([3])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70],])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# W = torch.zeros((3,1), requires_grad=True) \n",
    "W = torch.zeros(3, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "prediction = x_train.matmul(W) + b\n",
    "diff = prediction - y_train\n",
    "\n",
    "print(x_train.shape)\n",
    "print(W.shape)\n",
    "print(y_train.shape)\n",
    "print(prediction.shape)\n",
    "print(diff.shape) # broadcasting is not error for code.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module & pre-defined loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLR(nn.Module): # nn.Module 상속\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1) # (input, output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70],])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "model = MLR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4473, 0.4564, 0.2259]])\n",
      "tensor([0.1456])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.data)\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) # 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32.9862, 34.3992, 36.5326, 40.4243, 24.4337]) 19090.681640625\n",
      "tensor([ 86.9277,  99.2354, 100.4156, 109.9913,  73.8879]) 5992.08056640625\n",
      "tensor([117.1271, 135.5352, 136.1811, 148.9391, 101.5760]) 1886.3616943359375\n",
      "tensor([134.0340, 155.8585, 156.2047, 170.7445, 117.0781]) 599.4320678710938\n",
      "tensor([143.4991, 167.2371, 167.4151, 182.9524, 125.7577]) 196.0438995361328\n",
      "tensor([148.7976, 173.6080, 173.6912, 189.7870, 130.6176]) 69.59921264648438\n",
      "tensor([151.7635, 177.1752, 177.2047, 193.6132, 133.3389]) 29.9615478515625\n",
      "tensor([153.4235, 179.1727, 179.1717, 195.7553, 134.8631]) 17.53302001953125\n",
      "tensor([154.3523, 180.2915, 180.2728, 196.9544, 135.7169]) 13.633204460144043\n",
      "tensor([154.8717, 180.9182, 180.8890, 197.6256, 136.1954]) 12.406759262084961\n",
      "tensor([155.1619, 181.2695, 181.2339, 198.0013, 136.4639]) 12.018228530883789\n",
      "tensor([155.3239, 181.4665, 181.4268, 198.2115, 136.6147]) 11.892332077026367\n",
      "tensor([155.4140, 181.5772, 181.5346, 198.3290, 136.6996]) 11.848773002624512\n",
      "tensor([155.4639, 181.6396, 181.5948, 198.3946, 136.7477]) 11.831011772155762\n",
      "tensor([155.4912, 181.6749, 181.6283, 198.4313, 136.7752]) 11.821367263793945\n",
      "tensor([155.5060, 181.6950, 181.6470, 198.4516, 136.7911]) 11.814203262329102\n",
      "tensor([155.5137, 181.7067, 181.6572, 198.4629, 136.8005]) 11.807894706726074\n",
      "tensor([155.5175, 181.7136, 181.6628, 198.4691, 136.8063]) 11.801839828491211\n",
      "tensor([155.5190, 181.7178, 181.6657, 198.4724, 136.8100]) 11.795832633972168\n",
      "tensor([155.5193, 181.7206, 181.6672, 198.4741, 136.8126]) 11.789884567260742\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 20\n",
    "for epoch in range(n_epoch):\n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    print(prediction.squeeze().detach(), cost.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_huggingface",
   "language": "python",
   "name": "pytorch_huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
